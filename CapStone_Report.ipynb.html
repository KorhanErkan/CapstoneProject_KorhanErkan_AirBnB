<html>
<head>
<title>CapStone_Report.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
CapStone_Report.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1">**AirBnb HOUSE PRICE PREDICTION FOR NEW LISTINGS**         ///     **KORHAN ERKAN** 
</span><span class="s0">#%% md 
</span><span class="s1">ABSTRACT 
 
**AirBnb HOUSE PRICE PREDICTION FOR NEW LISTINGS** 
 
Price estimation applications are widely used today. People generally list their products after a superficial market research for the sale or rental of the product they have. By using machine learning techniques, the most suitable price for the product can be determined from listings that have been sold or rented with similar conditions and features before. This project based on AirBnB application which is widely used in many countries and data based on Seattle-USA AirBnB listings. The purpose of the project was to estimate the rental value of a new listing's home in this state. In this project, particular attention was paid to removing the columns in the data set that the new listers do not have. In the estimation phase, the Random Forest Classifier algorithm is used. 
 
</span><span class="s0">#%% md 
</span><span class="s1">Table Of Contents 
 
1.   **AirBnb House Price Prediction Challenge** 
 
2.   **Related Works** 
 
3.   **Data Preparation**  
 
4.   **Model Development** 
 
     4.1. Linear Regression 
     4.2. Logistic Regression 
     4.3. Ridge Regression 
     4.4. Lasso Regression 
     4.5. Decision Tree Classification 
     4.6. Random Forest Classification 
     4.7. K-Nearest Neighbours Algorithm 
 
 
5.   **Model Validation** 
 
6.   **Results and Discussion** 
 
7.   **Conclusion and Future Works** 
 
8.   **References** 
 
 
</span><span class="s0">#%% md 
</span><span class="s1">**INTRODUCTION** 
 
The purpose of this project is to estimate the price of the house for the person who will create a new AirBnB listing in Seattle, USA. In this estimation, the most appropriate price estimation is made by comparing it with other similar houses by taking advantage of the house type, interior features, location and amenities.The reason why I chose this project was that I often use the AirBnB application and in this way, I could have a better understanding of the data set. In addition, I believed that I could do a better job in the face of similar projects that I found as a result of my research on the internet. At the same time, inadequacies such as the lack of a mobile application of this study became one of the factors that increased my motivation. Thus, there were different areas where I could develop the project in the continuation of the project. I would also be able to apply many different information I learned in this course in this project and interpret the results. These reasons formed my main motivation for this project. 
</span><span class="s0">#%% md 
</span><span class="s1">1.   **AirBnb House Price Prediction Challenge** 
 
The AirBnB Seattle dataset consists of listings and a separate calendar dataset that shows at what date and at what price those listings were held. The calendar data set covers the last 4 months of 2022 and the 9 months of 2023. 
 
 
If I need to talk about the general difficulties of the project, since the AirBnb market is a free market like other sales channels, no outlier determination can be made, especially in the price part, absence of a few important data information such as cleaning fee in the dataset. In addition, the high number of columns that the new listings do not have has increased the importance of feature engineering in model development. 
</span><span class="s0">#%% md 
</span><span class="s1">2.   **Related Works** 
 
 
Yu and Wu **[1]** made an attempt to use feature significance analysis, linear regression, SVR, and Random Forest regression to predict real estate prices. With the use of Naive Bayes, Logistic Regression, SVC, and Random Forest, they also made an effort to categorize the prices into 7 groups. They stated that their SVR model had a best RMSE of 0.53 and that their SVC model with PCA had a classification accuracy of 69%. In a different study, Ma et al **[2]**. examined warehouse rental rates in Beijing using linear regression, regression trees, random forest regression, and gradient boosting regression trees. They came to the conclusion that the tree regression model performed the best, with an RMSE of 1.05 CNY(Chinese Yuan)/m2-day 
 
 
An Airbnb pricing prediction model was created by Yuanhang Luo, Xuanyu Zhou, and Yulian Zhou **[3]** utilizing Random Forest, XGBoost, and Neural Network. They kept features like country code and number of bedrooms to create the model while removing features like host id and customer name to reduce noise. Textual information is often referred to as textual features. Examples include house descriptions and neighborhood reviews. XGBoost and Neural Network outperform other models after rigorous feature engineering and extraction on the New York and Paris dataset. To analyze the outcome, R-Squared and Median Squared Error (MSE) are utilized. For XGBoost, the R-Squared error is 0.722, and for neural networks, it is 0.769. Other related efforts that used outside resources to add features were examined. Since longitude and zip code don't greatly effect prediction accuracy, they are typically removed from most research. However, QuangTrungNg **[4]** yen uses the location data to calculate the number of nearby tourist attractions in order to enhance the performance. All three of the aforementioned research used other elements in addition to the dataset's numerical data, including language, photos, and location. 
 
 
</span><span class="s0">#%% md 
</span><span class="s1">3. **Data Preparation** 
 
We select the dataset offered by Inside Airbnb [5], which includes broad details on listings and a calendar for several cities. We employ two split datasets in this experiment that contain Seattle booking data between September 2022 and September 2023. The first dataset, &quot;listings.csv,&quot; lists the essential details of each listing, like the host address, the type of lodging, the number of bedrooms, etc. Some of the data, such the host name and listing URL, have little bearing on the accuracy of our predictions. To lessen the loudness, they are deleted throughout the cleaning period. The second file, &quot;calendar.csv,&quot; includes availability, prices, and dates for each item. 
 
</span><span class="s0">#%% md 
</span><span class="s1">For the better understanding of datasets we need more information. It starts with calling the necessary libraries for data set review and preliminary preparation. 
</span><span class="s0">#%% md 
</span><span class="s1">For the detailed description of the columns please follow the URL link. 
 
[Detailed Description of The Columns](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=1322284596) 
 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">import </span><span class="s1">seaborn </span><span class="s2">as </span><span class="s1">sns</span>
<span class="s2">import </span><span class="s1">plotly.express </span><span class="s2">as </span><span class="s1">px</span>
<span class="s0">#%% 
#Reading datasets</span>
<span class="s1">listings_df = pd.read_csv(</span><span class="s3">&quot;listings.csv&quot;</span><span class="s1">)</span>
<span class="s1">dataC = pd.read_csv(</span><span class="s3">&quot;calendar.csv&quot;</span><span class="s1">)</span>
<span class="s0">#Displaying all columns</span>
<span class="s1">pd.set_option(</span><span class="s3">'display.max_columns'</span><span class="s2">, None</span><span class="s1">)</span>
<span class="s0">#%% 
#Checking shapes of dataframes</span>
<span class="s1">print(</span><span class="s3">&quot;Listings DataFrame Shape:&quot;</span><span class="s2">, </span><span class="s1">listings_df.shape)</span>
<span class="s1">print(</span><span class="s3">&quot;Calendar DataFrame Shape:&quot;</span><span class="s2">, </span><span class="s1">dataC.shape)</span>
<span class="s0">#%% 
</span><span class="s1">listings_df.head(</span><span class="s4">3</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">dataC.head()</span>
<span class="s0">#%% md 
</span><span class="s1">After getting close look on both dataframes based on columns and shapes we can get into Data Preparation Stages. 
</span><span class="s0">#%% md 
</span><span class="s1">3.1.**Data Preparation Stages** 
 
In this section we will prepare the data for the machine learning models. First, columns with no interest to target should be deleted. 
 
</span><span class="s0">#%% 
</span><span class="s1">listings_df.columns</span>
<span class="s0">#%% 
#Finding irrelevant columns and deleting them.</span>
<span class="s1">irrelevant = [</span><span class="s3">&quot;scrape_id&quot;</span><span class="s2">,</span><span class="s3">&quot;last_scraped&quot;</span><span class="s2">, </span><span class="s3">&quot;source&quot;</span><span class="s2">, </span><span class="s3">&quot;name&quot;</span><span class="s2">,</span><span class="s3">&quot;picture_url&quot;</span><span class="s2">,</span><span class="s3">&quot;host_id&quot;</span><span class="s2">,</span><span class="s3">&quot;host_url&quot;</span><span class="s2">, </span><span class="s3">&quot;host_name&quot;</span><span class="s2">,</span>
              <span class="s3">&quot;host_location&quot;</span><span class="s2">, </span><span class="s3">&quot;host_about&quot;</span><span class="s2">,</span><span class="s3">&quot;host_thumbnail_url&quot;</span><span class="s2">, </span><span class="s3">&quot;host_picture_url&quot;</span><span class="s2">, </span><span class="s3">&quot;host_neighbourhood&quot;</span><span class="s2">, </span><span class="s3">'calendar_last_scraped'</span><span class="s2">, </span><span class="s3">&quot;host_thumbnail_url&quot;</span><span class="s2">,	</span><span class="s3">&quot;host_picture_url&quot;</span><span class="s2">, </span>
              <span class="s3">&quot;host_neighbourhood&quot;</span><span class="s2">, </span><span class="s3">&quot;description&quot;</span><span class="s1">]</span>

<span class="s1">listings_df = listings_df.drop(columns = irrelevant)</span>
<span class="s0">#%% md 
</span><span class="s1">After the deletion of irrelevant columns, we should also determine and delete the columns that someone who will make a new listing won't have 
</span><span class="s0">#%% 
#Finding columns that new listings can't have and deleting them.</span>
<span class="s1">no_newbie = [</span><span class="s3">'host_since'</span><span class="s2">, </span><span class="s3">'host_response_time'</span><span class="s2">, </span><span class="s3">'host_response_rate'</span><span class="s2">, </span><span class="s3">'host_acceptance_rate'</span><span class="s2">, </span><span class="s3">'host_is_superhost'</span><span class="s2">,</span><span class="s3">'number_of_reviews'</span><span class="s2">,</span>
             <span class="s3">'number_of_reviews_ltm'</span><span class="s2">, </span><span class="s3">'number_of_reviews_l30d'</span><span class="s2">, </span><span class="s3">'first_review'</span><span class="s2">, </span><span class="s3">'last_review'</span><span class="s2">, </span><span class="s3">'review_scores_rating'</span><span class="s2">, </span><span class="s3">'review_scores_accuracy'</span><span class="s2">,</span>
             <span class="s3">'review_scores_cleanliness'</span><span class="s2">, </span><span class="s3">'review_scores_checkin'</span><span class="s2">, </span><span class="s3">'review_scores_communication'</span><span class="s2">, </span><span class="s3">'review_scores_location'</span><span class="s2">,</span>
             <span class="s3">'review_scores_value'</span><span class="s2">, </span><span class="s3">'reviews_per_month'</span><span class="s1">]</span>

<span class="s1">listings_df = listings_df.drop(columns = no_newbie)</span>
<span class="s0">#%% 
#We have cleansed neighbourhood. Deleting ones with extensive informations</span>
<span class="s1">neighbourhood = [</span><span class="s3">&quot;neighbourhood&quot;</span><span class="s2">,	</span><span class="s3">&quot;neighborhood_overview&quot;</span><span class="s1">]</span>
<span class="s1">listings_df = listings_df.drop(columns = neighbourhood)</span>
<span class="s0">#%% 
#Function for deleting $ for our target column</span>
<span class="s2">def </span><span class="s1">financial_to_float(x):</span>
    <span class="s1">x = str(x).replace(</span><span class="s3">&quot;,&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">float(x.strip(</span><span class="s3">'$'</span><span class="s1">))</span>

<span class="s1">listings_df[</span><span class="s3">'price'</span><span class="s1">] = listings_df[</span><span class="s3">'price'</span><span class="s1">].apply(financial_to_float)</span>
<span class="s0">#%% md 
</span><span class="s1">Checking for NaN values for listings dataframe 
</span><span class="s0">#%% 
</span><span class="s1">listings_df.isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">There is a column named calendar_updated and bathrooms with all NaN values. We can get value for bathrooms from bathrooms_text but there is nothing we can do for calendar_updated. 
</span><span class="s0">#%% 
#Deleting all NaN column</span>
<span class="s1">all_nan = [</span><span class="s3">&quot;calendar_updated&quot;</span><span class="s1">]</span>
<span class="s1">listings_df = listings_df.drop(columns = all_nan)</span>
<span class="s1">listings_df = listings_df[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].notna()]</span>
<span class="s0">#%% 
#Dropping column with low numbers</span>
<span class="s1">listings_df = listings_df[listings_df[</span><span class="s3">'minimum_minimum_nights'</span><span class="s1">].notna()]</span>
<span class="s0">#%% md 
</span><span class="s1">Getting values for bathrooms from bathrooms_text. After that we can delete bathrooms_text column because it must be numerical column. 
</span><span class="s0">#%% 
</span><span class="s1">listings_df</span>
<span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">&quot;bathrooms_text&quot;</span><span class="s1">].isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">Making bathrooms_text string values to float and integers. 
</span><span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">] = listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].fillna(</span><span class="s3">&quot;0 baths&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;1 bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;2 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">2</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;1 shared bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;1 private bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;2.5 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">2.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;1.5 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;3 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">3</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;3.5 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">3.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;2 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">2</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;1.5 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;3 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">3</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;4 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">4</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;0 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">0</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Half-bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">0.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;2.5 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">2.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;4.5 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">4.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;0 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">0</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;3.5 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">3.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;6 baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">6</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;4 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">4</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Shared half-bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">0.5</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;16 shared baths&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">1</span>
<span class="s1">listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].loc[listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Private half-bath&quot;</span><span class="s1">).fillna(</span><span class="s2">False</span><span class="s1">)] = </span><span class="s4">0.5</span>
<span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'bathrooms'</span><span class="s1">] = listings_df[</span><span class="s3">'bathrooms_text'</span><span class="s1">]</span>
<span class="s1">listings_df = listings_df.drop(columns = </span><span class="s3">&quot;bathrooms_text&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">listings_df.shape</span>
<span class="s0">#%% md 
</span><span class="s1">Checkin nan values of bedrooms and beds. 
</span><span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'bedrooms'</span><span class="s1">] = listings_df[</span><span class="s3">'bedrooms'</span><span class="s1">].fillna(</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">listings_df[</span><span class="s3">'beds'</span><span class="s1">] = listings_df[</span><span class="s3">'beds'</span><span class="s1">].fillna(</span><span class="s4">1</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">] = listings_df[</span><span class="s3">'license'</span><span class="s1">].fillna(</span><span class="s3">&quot;f&quot;</span><span class="s1">)</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Exempt&quot;</span><span class="s1">)] = </span><span class="s3">&quot;f&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;City registration pending&quot;</span><span class="s1">)] = </span><span class="s3">&quot;f&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;STR&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Str&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;str&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Approved by government&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;19&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;8&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;7&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;5&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s1">listings_df[</span><span class="s3">'license'</span><span class="s1">].loc[listings_df[</span><span class="s3">'license'</span><span class="s1">].str.contains(</span><span class="s3">&quot;6&quot;</span><span class="s1">)] = </span><span class="s3">&quot;t&quot;</span>
<span class="s0">#%% md 
</span><span class="s1">Adding new column based on amenities and verifications. 
</span><span class="s0">#%% 
</span><span class="s1">listings_df[</span><span class="s3">'number_of_amenities'</span><span class="s1">]  = listings_df[</span><span class="s3">'amenities'</span><span class="s1">].str.split(</span><span class="s3">&quot;,&quot;</span><span class="s1">).str.len()</span>
<span class="s1">listings_df[</span><span class="s3">'number_of_verifications'</span><span class="s1">]  = listings_df[</span><span class="s3">'host_verifications'</span><span class="s1">].str.split(</span><span class="s3">&quot;,&quot;</span><span class="s1">).str.len()</span>
<span class="s1">listings_df = listings_df.drop(columns = </span><span class="s3">&quot;amenities&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">There should be no NaN values after cleaning. 
</span><span class="s0">#%% 
</span><span class="s1">listings_df.isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">We can look for outliers now. 
</span><span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;accommodates&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">10</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;bathrooms&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">10</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;beds&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">7</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;bedrooms&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">6</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;number_of_amenities&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">6</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">plt.figure(figsize = (</span><span class="s4">10</span><span class="s2">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.histplot(listings_df[</span><span class="s3">&quot;number_of_verifications&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins = </span><span class="s4">3</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">As you can see there is lots of data that we can say outlier. But before doing that we should remember city of Seattle has all kinds of houses. Flats, lofts and big town houses. After reviewing the candidate outliers, we realize that they are not actually outliers and that they are unique listings. 
</span><span class="s0">#%% md 
</span><span class="s1">We can move on to calendar data. 
</span><span class="s0">#%% 
</span><span class="s1">dataC.head()</span>
<span class="s0">#%% md 
</span><span class="s1">As you can see we have fewer columns. We have dates and prices columns according to that date. And availability information. 
</span><span class="s0">#%% 
#Checking for empty columns</span>
<span class="s1">dataC.isnull().sum()</span>
<span class="s0">#%% 
#We can delete minimum_nights and maximum_nights columns because we have same columns in listings_df</span>
<span class="s1">same_ones = [</span><span class="s3">&quot;minimum_nights&quot;</span><span class="s2">, </span><span class="s3">&quot;maximum_nights&quot;</span><span class="s1">]</span>

<span class="s1">dataC = dataC.drop(columns = same_ones</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s0">#%% 
#Diving general date information for better understanding and visualization.</span>
<span class="s2">import </span><span class="s1">datetime </span><span class="s2">as </span><span class="s1">dt</span>

<span class="s1">dataC[</span><span class="s3">'date'</span><span class="s1">] = pd.to_datetime(dataC[</span><span class="s3">'date'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">format = </span><span class="s3">'%Y-%m-%d'</span><span class="s1">)</span>
<span class="s1">dataC[</span><span class="s3">'month'</span><span class="s1">] = dataC[</span><span class="s3">'date'</span><span class="s1">].dt.month</span>
<span class="s1">dataC[</span><span class="s3">'year'</span><span class="s1">] = dataC[</span><span class="s3">'date'</span><span class="s1">].dt.year</span>
<span class="s1">dataC[</span><span class="s3">'day'</span><span class="s1">] = dataC[</span><span class="s3">'date'</span><span class="s1">].dt.day</span>

<span class="s0">#%% md 
</span><span class="s1">Before processing the NaN values ​​in the price column, we must merge it with the listings_df file. In this way, we can perform more precise operations. 
</span><span class="s0">#%% 
#Merging</span>
<span class="s1">listings_df = listings_df.rename(columns={</span><span class="s3">&quot;id&quot;</span><span class="s1">: </span><span class="s3">&quot;listing_id&quot;</span><span class="s1">})</span>
<span class="s1">full_df = pd.merge(dataC</span><span class="s2">, </span><span class="s1">listings_df</span><span class="s2">, </span><span class="s1">on = </span><span class="s3">'listing_id'</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">full_df.head()</span>
<span class="s0">#%% 
</span><span class="s1">full_df.shape</span>
<span class="s0">#%% md 
</span><span class="s1">Closer look for price columns, our goal is filling NaN values best way possible. 
</span><span class="s0">#%% 
</span><span class="s1">full_df[</span><span class="s3">'price_x'</span><span class="s1">] = full_df[</span><span class="s3">'price_x'</span><span class="s1">].apply(financial_to_float)</span>
<span class="s1">full_df[</span><span class="s3">'adjusted_price'</span><span class="s1">] = full_df[</span><span class="s3">'adjusted_price'</span><span class="s1">].apply(financial_to_float)</span>
<span class="s0">#%% 
#Filling NaN price_x values with price_y values</span>
<span class="s1">full_df.price_x.fillna(full_df.price_y</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">full_df = full_df.drop(columns=</span><span class="s3">&quot;adjusted_price&quot;</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">full_df.isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">Visualizations 
</span><span class="s0">#%% 
</span><span class="s1">monthly_booked = full_df[(full_df[</span><span class="s3">'available'</span><span class="s1">] == </span><span class="s3">'f'</span><span class="s1">) &amp; (full_df[</span><span class="s3">'year'</span><span class="s1">] == </span><span class="s4">2022</span><span class="s1">)][[</span><span class="s3">'listing_id'</span><span class="s2">, </span><span class="s3">'month'</span><span class="s1">]].groupby([</span><span class="s3">'month'</span><span class="s1">]).count()</span>
<span class="s1">monthly_booked_1 = full_df[(full_df[</span><span class="s3">'available'</span><span class="s1">] == </span><span class="s3">'f'</span><span class="s1">) &amp; (full_df[</span><span class="s3">'year'</span><span class="s1">] == </span><span class="s4">2023</span><span class="s1">)][[</span><span class="s3">'listing_id'</span><span class="s2">, </span><span class="s3">'month'</span><span class="s1">]].groupby([</span><span class="s3">'month'</span><span class="s1">]).count()</span>
<span class="s1">monthly_booked = monthly_booked.reset_index()</span>
<span class="s1">monthly_booked.rename(columns={</span><span class="s3">&quot;listing_id&quot;</span><span class="s1">: </span><span class="s3">&quot;booked_listings&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">inplace = </span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">monthly_booked_1 = monthly_booked_1.reset_index()</span>
<span class="s1">monthly_booked_1.rename(columns={</span><span class="s3">&quot;listing_id&quot;</span><span class="s1">: </span><span class="s3">&quot;booked_listings&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">inplace = </span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">sns.catplot (x = </span><span class="s3">'month'</span><span class="s2">, </span><span class="s1">y = </span><span class="s3">'booked_listings'</span><span class="s2">, </span><span class="s1">data = monthly_booked </span><span class="s2">, </span><span class="s1">kind = </span><span class="s3">'bar'</span><span class="s1">)</span>
<span class="s1">sns.catplot (x = </span><span class="s3">'month'</span><span class="s2">, </span><span class="s1">y = </span><span class="s3">'booked_listings'</span><span class="s2">, </span><span class="s1">data = monthly_booked_1 </span><span class="s2">, </span><span class="s1">kind = </span><span class="s3">'bar'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">We can see listing numbers for each month. First graph represents 2022 and second graph represents 2023. We can see June has most listings and Feburary has least. 
</span><span class="s0">#%% 
</span><span class="s1">sns.set(rc={</span><span class="s3">'figure.figsize'</span><span class="s1">:(</span><span class="s4">11.7</span><span class="s2">,</span><span class="s4">8.27</span><span class="s1">)})</span>

<span class="s1">avg_neighbourhood_price = full_df[[</span><span class="s3">'neighbourhood_group_cleansed'</span><span class="s2">,</span><span class="s3">'price_x'</span><span class="s1">]].groupby(</span><span class="s3">'neighbourhood_group_cleansed'</span><span class="s1">).mean().reset_index()</span>

<span class="s1">top_20_neighbourhoods = avg_neighbourhood_price.nlargest(</span><span class="s4">20</span><span class="s2">, </span><span class="s3">'price_x'</span><span class="s2">, </span><span class="s1">keep=</span><span class="s3">'first'</span><span class="s1">)</span>
<span class="s1">top_20_neighbourhoods.rename(columns={</span><span class="s3">&quot;price_x&quot;</span><span class="s1">: </span><span class="s3">&quot;avg_price&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">inplace = </span><span class="s2">True</span><span class="s1">)</span>

<span class="s1">sns.catplot (x = </span><span class="s3">'neighbourhood_group_cleansed'</span><span class="s2">, </span><span class="s1">y = </span><span class="s3">'avg_price'</span><span class="s2">, </span><span class="s1">data = top_20_neighbourhoods </span><span class="s2">, </span><span class="s1">kind = </span><span class="s3">'bar'</span><span class="s1">)</span>
<span class="s1">plt.xticks(rotation=</span><span class="s4">90</span><span class="s1">);</span>
<span class="s0">#%% md 
</span><span class="s1">Here we can see most expensive and least expensive neighbourhood according to our data. 
</span><span class="s0">#%% 
#Dropping other useless columns</span>
<span class="s1">columns = [</span><span class="s3">&quot;date&quot;</span><span class="s2">, </span><span class="s3">&quot;price_y&quot;</span><span class="s2">, </span><span class="s3">&quot;listing_url&quot;</span><span class="s2">, </span><span class="s3">&quot;listing_id&quot;</span><span class="s2">, </span><span class="s3">&quot;latitude&quot;</span><span class="s2">, </span><span class="s3">&quot;longitude&quot;</span><span class="s1">]</span>
<span class="s1">full_df = full_df.drop(columns = columns</span><span class="s2">, </span><span class="s1">axis = </span><span class="s4">1</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">sns.heatmap(full_df.corr())</span>
<span class="s0">#%% 
</span><span class="s1">full_df.describe()</span>
<span class="s0">#%% md 
</span><span class="s1">4. Model Development 
 
Supervised learning will be used for model develepment. We will find categorical variables for one hot encoding: 
 
 
</span><span class="s0">#%% 
</span><span class="s1">cat_col = [</span><span class="s3">&quot;available&quot;</span><span class="s2">, </span><span class="s3">&quot;host_verifications&quot;</span><span class="s2">, </span><span class="s3">&quot;host_has_profile_pic&quot;</span><span class="s2">, </span><span class="s3">&quot;host_identity_verified&quot;</span><span class="s2">, </span><span class="s3">&quot;neighbourhood_cleansed&quot;</span><span class="s2">, </span><span class="s3">&quot;neighbourhood_group_cleansed&quot;</span><span class="s2">, </span><span class="s3">&quot;property_type&quot;</span><span class="s2">, </span><span class="s3">&quot;room_type&quot;</span><span class="s2">, </span><span class="s3">&quot;license&quot;</span><span class="s2">, </span><span class="s3">&quot;instant_bookable&quot;</span><span class="s2">, </span><span class="s3">&quot;has_availability&quot;</span><span class="s1">]</span>
<span class="s0">#%% 
</span><span class="s1">full_df = pd.get_dummies(full_df</span><span class="s2">, </span><span class="s1">columns= cat_col)</span>
<span class="s0">#%% 
</span><span class="s1">full_df.shape</span>
<span class="s0">#%% 
</span><span class="s1">full_df.isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">After preparing data it is time to develop different models. After finding the most successful model we will get closer look. 
</span><span class="s0">#%% 
</span><span class="s1">full_df.head()</span>
<span class="s0">#%% md 
</span><span class="s1">Since my dataset is too big to compute (with the specs that i have) i will take the random 100000 rows from my main dataset and apply machine learning algorithms. After finding best model i will continue with full_df. 
</span><span class="s0">#%% 
</span><span class="s1">lf = full_df.sample(</span><span class="s4">100000</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">lf.shape</span>
<span class="s0">#%% md 
</span><span class="s1">4.1. **Linear Regression** 
 
The link between a dependent variable and an independent (predictor) variable is modeled using a machine learning technique called linear regression. The model can employ linear predictor functions to model the relationship and estimate unknown parameters from the data. In order to determine whether the independent variables are useful in price prediction, linear regression is used. For instance, a larger house costs more money. Some connections, like the one between longitude and cost, are not linear, though. Not only in two-dimensional space, but also in multidimensional space, are linear relationships possible. The price can be predicted using a linear regression model based on previous data if there is a linear relationship between two or more factors. 
 
Values in X and Y reflect the input (dependent) variable and the output (independent) variable, respectively, that the model is attempting to predict, given two datasets. In our situation, we can assume that X represents the accommodates or the number of beds, and Y represents the cost of the matching property. The best model to predict the value of Y is linear regression if each value in X is increasing as each value in Y. If this is the case, then X and Y may have a linear connection. 
 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">warnings</span>
<span class="s1">warnings.simplefilter(action=</span><span class="s3">'ignore'</span><span class="s2">, </span><span class="s1">category=FutureWarning)</span>
<span class="s1">warnings.filterwarnings(</span><span class="s3">&quot;ignore&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">r2_score</span><span class="s2">, </span><span class="s1">mean_squared_error</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span>
<span class="s1">X = lf.drop([</span><span class="s3">'price_x'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">y = lf[</span><span class="s3">'price_x'</span><span class="s1">].copy()</span>

<span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">.30</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s1">lm_model = LinearRegression(normalize=</span><span class="s2">True</span><span class="s1">)  </span><span class="s0"># Instantiate</span>
<span class="s1">lm_model.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)  </span><span class="s0">#Fit</span>

<span class="s1">y_test_preds = lm_model.predict(X_test)</span>

<span class="s3">&quot;The r-squared score for your model was {} on {} values.&quot;</span><span class="s1">.format(r2_score(y_test</span><span class="s2">, </span><span class="s1">y_test_preds)</span><span class="s2">, </span><span class="s1">len(y_test))</span>

<span class="s1">lin_acc = r2_score(y_test</span><span class="s2">, </span><span class="s1">y_test_preds)</span>
<span class="s1">print(lin_acc)</span>
<span class="s0">#%% md 
</span><span class="s1">4.2. **Logistic Regression** 
 
A statistical analysis method called logistic regression uses previous observations from a data set to predict a binary outcome, such as yes or no. By examining the correlation between one or more already present independent variables, a logistic regression model forecasts a dependent data variable. A logistic regression could be used, for instance, to forecast whether a candidate for office will win or lose, or if a high school student will be accepted into a particular institution or not. These simple choices between two options allow for binary outcomes. Multiple criteria for input can be taken into account using a logistic regression model. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LogisticRegression</span>
<span class="s1">logreg = LogisticRegression()</span>
<span class="s1">logreg.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
<span class="s1">y_pred = logreg.predict(X_test)</span>
<span class="s3">&quot;The r-squared score for your model was {} on {} values.&quot;</span><span class="s1">.format(r2_score(y_test</span><span class="s2">, </span><span class="s1">y_pred)</span><span class="s2">, </span><span class="s1">len(y_test))</span>

<span class="s1">lr_acc = r2_score(y_test</span><span class="s2">, </span><span class="s1">y_pred)</span>
<span class="s1">print(lr_acc)</span>
<span class="s0">#%% md 
</span><span class="s1">4.3. **Ridge Regression** 
 
The multicollinearity issue in linear regression is addressed by ridge regression, commonly known as Tikhonov Regularization. When a predictor variable is predicted from other predictors, the multicollinearity problem arises. The weight of a single predictor may not be accurate, even while it does not affect the group's overall prediction accuracy. When the model contains a significant number of features, the multicollinearity problem frequently arises. Weight values will be quite high when calculating parameter weights using the least squares approach in the multicollinearity case. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">Ridge</span>
<span class="s1">ridge_reg = Ridge()</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span>
<span class="s1">params_Ridge = {</span><span class="s3">'alpha'</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">,</span><span class="s4">0.1</span><span class="s2">,</span><span class="s4">0.01</span><span class="s2">,</span><span class="s4">0.01</span><span class="s1">] </span><span class="s2">, </span><span class="s3">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s2">True, False</span><span class="s1">]</span><span class="s2">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">: [</span><span class="s3">'svd'</span><span class="s2">, </span><span class="s3">'cholesky'</span><span class="s1">]}</span>
<span class="s1">Ridge_GS = GridSearchCV(ridge_reg</span><span class="s2">, </span><span class="s1">param_grid=params_Ridge</span><span class="s2">, </span><span class="s1">n_jobs=-</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">result = Ridge_GS.fit(X_train</span><span class="s2">,</span><span class="s1">y_train)</span>
<span class="s1">Ridge_GS.best_params_</span>
<span class="s1">print(</span><span class="s3">'Best Score: %s' </span><span class="s1">% result.best_score_)</span>

<span class="s1">r_acc = result.best_score_</span>
<span class="s1">print(r_acc)</span>
<span class="s0">#%% md 
</span><span class="s1">4.4. **Lasso Regression** 
 
Shrinkage is used in the linear regression method known as lasso regression. When data values shrink toward a middle value, such as the mean, this is called shrinkage. Simple, sparse models are encouraged by the lasso approach (i.e. models with fewer parameters). When models exhibit significant levels of multicollinearity or when you wish to automate specific steps in the model selection process, such as variable selection and parameter elimination, this particular sort of regression is ideally suited. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">Lasso</span>
<span class="s1">model = Lasso()</span>
<span class="s1">lasso_p = {</span><span class="s3">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s2">True, False</span><span class="s1">]</span><span class="s2">,</span>
           <span class="s3">&quot;alpha&quot;</span><span class="s1">: np.arange(</span><span class="s4">1</span><span class="s2">,</span><span class="s4">5</span><span class="s1">)</span><span class="s2">,</span>
           <span class="s3">&quot;max_iter&quot;</span><span class="s1">: np.arange(</span><span class="s4">2</span><span class="s2">,</span><span class="s4">5</span><span class="s1">)}</span>
<span class="s1">l_gr = GridSearchCV(model</span><span class="s2">,</span><span class="s1">lasso_p</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
<span class="s1">l_gr.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
<span class="s1">print(</span><span class="s3">&quot;Score&quot;</span><span class="s2">,</span><span class="s1">l_gr.best_score_)</span>

<span class="s1">l_acc = l_gr.best_score_</span>
<span class="s1">print(l_acc)</span>
<span class="s0">#%% md 
</span><span class="s1">4.5. **Decision Tree Classification** 
 
A decision tree creates tree-like models for classification or regression. It incrementally develops an associated decision tree while segmenting a dataset into smaller and smaller sections. The outcome is a tree containing leaf nodes and decision nodes. Two or more branches, one for each value of the characteristic under test, make up a decision node. A choice regarding the numerical aim is represented by a leaf node. The root node is the topmost decision node in a tree and corresponds to the best predictor. Both category and numerical data can be processed using decision trees. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeClassifier</span>
<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">metrics</span>

<span class="s1">model = DecisionTreeClassifier()</span>
<span class="s1">model.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

<span class="s1">param_dict = {</span>
    <span class="s3">&quot;criterion&quot;</span><span class="s1">: [</span><span class="s3">&quot;gini&quot;</span><span class="s2">, </span><span class="s3">&quot;entropy&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s3">&quot;max_depth&quot;</span><span class="s1">:range(</span><span class="s4">5</span><span class="s2">,</span><span class="s4">10</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s3">&quot;min_samples_split&quot;</span><span class="s1">:range(</span><span class="s4">5</span><span class="s2">,</span><span class="s4">10</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s3">&quot;min_samples_leaf&quot;</span><span class="s1">:range(</span><span class="s4">5</span><span class="s2">,</span><span class="s4">10</span><span class="s1">)</span>
<span class="s1">}</span>

<span class="s1">grid = GridSearchCV(model</span><span class="s2">,</span>
                    <span class="s1">param_grid=param_dict</span><span class="s2">,</span>
                    <span class="s1">cv=</span><span class="s4">5</span><span class="s2">,</span>
                    <span class="s1">verbose=</span><span class="s4">1</span><span class="s2">,</span>
                    <span class="s1">n_jobs=-</span><span class="s4">1</span><span class="s1">)</span>

<span class="s1">grid.fit(X_train</span><span class="s2">,</span><span class="s1">y_train)</span>
<span class="s1">grid.best_params_</span>
<span class="s1">grid.best_estimator_</span>
<span class="s1">print(grid.best_score_)</span>

<span class="s1">dtc_acc = grid.best_score_</span>
<span class="s0">#%% md 
</span><span class="s1">4.6. **Random Forest Classifier** 
 
The bagging method is extended by the random forest algorithm, which uses feature randomness in addition to bagging to produce an uncorrelated forest of decision trees. The random subspace method, also known as feature bagging, creates a random subset of features that guarantees low correlation between decision trees. The main distinction between decision trees and random forests is this. Random forests merely choose a portion of those feature splits, whereas decision trees take into account all possible feature splits. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span>


<span class="s1">param_grid = [</span>
    <span class="s1">{</span><span class="s3">'n_estimators'</span><span class="s1">:[</span><span class="s4">3</span><span class="s2">,</span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span><span class="s3">'max_features'</span><span class="s1">:[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">4</span><span class="s2">,</span><span class="s4">6</span><span class="s1">]}</span><span class="s2">,</span>
    <span class="s1">{</span><span class="s3">'bootstrap'</span><span class="s1">:[</span><span class="s2">False</span><span class="s1">]</span><span class="s2">,</span><span class="s3">'n_estimators'</span><span class="s1">:[</span><span class="s4">3</span><span class="s2">,</span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s3">'max_features'</span><span class="s1">:[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">3</span><span class="s1">]}</span>
<span class="s1">]</span>

<span class="s1">forest_reg = RandomForestRegressor()</span>

<span class="s1">grid_search = GridSearchCV(forest_reg</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">cv = </span><span class="s4">5</span><span class="s2">,</span>
                           <span class="s1">scoring = </span><span class="s3">'neg_mean_squared_error'</span><span class="s2">,</span>
                           <span class="s1">return_train_score = </span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">grid_search.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

<span class="s1">grid_search.best_estimator_</span>

<span class="s1">cvres = grid_search.cv_results_</span>

<span class="s1">y_test_preds = grid_search.predict(X_test)</span>
<span class="s1">y_train_preds = grid_search.predict(X_train)</span>

<span class="s1">print(</span><span class="s3">&quot;The r-squared score for your model was {} on {} values.&quot;</span><span class="s1">.format(r2_score(y_test</span><span class="s2">, </span><span class="s1">y_test_preds)</span><span class="s2">, </span><span class="s1">len(y_test)))</span>

<span class="s1">rfc_acc = r2_score(y_test</span><span class="s2">, </span><span class="s1">y_test_preds)</span>
<span class="s0">#%% md 
</span><span class="s1">4.7. **K-Nearest Neighbors Algorithm** 
 
The k-nearest neighbors algorithm, sometimes referred to as KNN or k-NN, is a supervised learning classifier that employs proximity to produce classifications or predictions about the grouping of a single data point. Although it can be applied to classification or regression issues, it is commonly employed as a classification algorithm because it relies on the idea that comparable points can be discovered close to one another. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">accuracy_score</span><span class="s2">, </span><span class="s1">plot_confusion_matrix</span>
<span class="s1">knn = KNeighborsClassifier()</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span>
<span class="s1">k_range = list(range(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">31</span><span class="s1">))</span>
<span class="s1">param_grid = dict(n_neighbors=k_range)</span>

<span class="s0"># defining parameter range</span>
<span class="s1">grid = GridSearchCV(knn</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s3">'accuracy'</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">False,</span><span class="s1">verbose=</span><span class="s4">1</span><span class="s1">)</span>

<span class="s0"># fitting the model for grid search</span>
<span class="s1">grid_search=grid.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
<span class="s1">accuracy = grid_search.best_score_ *</span><span class="s4">100</span>
<span class="s1">print(</span><span class="s3">&quot;Accuracy for our training dataset with tuning is : {:.2f}%&quot;</span><span class="s1">.format(accuracy) )</span>

<span class="s1">knn_acc = grid_search.best_score_ *</span><span class="s4">100</span>
<span class="s0">#%% 
</span><span class="s1">data = [[</span><span class="s3">'LinearRegression'</span><span class="s2">, </span><span class="s1">lin_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'LogisticRegression'</span><span class="s2">, </span><span class="s1">lr_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'Ridge'</span><span class="s2">, </span><span class="s1">r_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'Lasso'</span><span class="s2">, </span><span class="s1">l_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'DecisionTreeClassifier'</span><span class="s2">, </span><span class="s1">dtc_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'RandomForestRegressor'</span><span class="s2">, </span><span class="s1">rfc_acc]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'KNeighborsClassifier'</span><span class="s2">, </span><span class="s1">knn_acc/</span><span class="s4">100</span><span class="s1">]]</span>
<span class="s1">df = pd.DataFrame(data</span><span class="s2">, </span><span class="s1">columns=[</span><span class="s3">'Name'</span><span class="s2">, </span><span class="s3">'Accuracy'</span><span class="s1">])</span>
<span class="s0">#%% 
</span><span class="s1">df</span>
<span class="s0">#%% md 
</span><span class="s1">As we can see even with part of our data RandomForestRegressor is best for our model by far. 
</span><span class="s0">#%% md 
</span><span class="s1">5.   **Model Validation** 
 
Now we can proceed with RandomForestRegressor and full_data 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">r2_score</span><span class="s2">, </span><span class="s1">mean_squared_error</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestRegressor</span>
<span class="s1">X = full_df.drop([</span><span class="s3">'price_x'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">y = full_df[</span><span class="s3">'price_x'</span><span class="s1">].copy()</span>

<span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">.30</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s1">param_grid = [</span>
    <span class="s1">{</span><span class="s3">'n_estimators'</span><span class="s1">:[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">3</span><span class="s2">,</span><span class="s4">4</span><span class="s2">,</span><span class="s4">5</span><span class="s1">]</span><span class="s2">,</span><span class="s3">'max_features'</span><span class="s1">:[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">3</span><span class="s2">,</span><span class="s4">4</span><span class="s2">,</span><span class="s4">5</span><span class="s1">]}</span><span class="s2">,</span>
    <span class="s1">{</span><span class="s3">'bootstrap'</span><span class="s1">:[</span><span class="s2">False</span><span class="s1">]</span><span class="s2">,</span><span class="s3">'n_estimators'</span><span class="s1">:[</span><span class="s4">3</span><span class="s2">,</span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s3">'max_features'</span><span class="s1">:[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">3</span><span class="s1">]}</span>
<span class="s1">]</span>

<span class="s1">forest_reg = RandomForestRegressor()</span>

<span class="s1">grid_search = GridSearchCV(forest_reg</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">cv = </span><span class="s4">10</span><span class="s2">,</span>
                           <span class="s1">scoring = </span><span class="s3">'neg_mean_squared_error'</span><span class="s2">,</span>
                           <span class="s1">return_train_score = </span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">grid_search.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

<span class="s1">grid_search.best_estimator_</span>

<span class="s1">cvres = grid_search.cv_results_</span>

<span class="s1">y_test_preds = grid_search.predict(X_test)</span>
<span class="s1">y_train_preds = grid_search.predict(X_train)</span>

<span class="s1">print(</span><span class="s3">&quot;The r-squared score for your model was {} on {} values.&quot;</span><span class="s1">.format(r2_score(y_test</span><span class="s2">, </span><span class="s1">y_test_preds)</span><span class="s2">, </span><span class="s1">len(y_test)))</span>
<span class="s0">#%% md 
</span><span class="s1">After we adjust our gridsearch parameters better our score increases as well. Real purpose of using GridSearch and Cross Validation is avoid overfit. 
</span><span class="s0">#%% 
#Testing for overfitting</span>
<span class="s3">&quot;The r-squared (train) score for your model was {} on {} values.&quot;</span><span class="s1">.format(r2_score(y_train</span><span class="s2">, </span><span class="s1">y_train_preds)</span><span class="s2">, </span><span class="s1">len(y_test))</span>
<span class="s0">#%% 
</span><span class="s1">print(</span><span class="s3">&quot;</span><span class="s2">\n </span><span class="s3">The best parameters across ALL searched params:</span><span class="s2">\n</span><span class="s3">&quot;</span><span class="s2">, </span><span class="s1">grid_search.best_params_)</span>
<span class="s1">feature_importances = grid_search.best_estimator_.feature_importances_</span>
<span class="s1">attributes = X_train.columns</span>
<span class="s1">values = sorted(zip(attributes</span><span class="s2">, </span><span class="s1">feature_importances)</span><span class="s2">, </span><span class="s1">reverse=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">rdm_forest_feature_importance = pd.DataFrame(values</span><span class="s2">, </span><span class="s1">columns=[</span><span class="s3">&quot;feature&quot;</span><span class="s2">, </span><span class="s3">&quot;score&quot;</span><span class="s1">]).sort_values(by=[</span><span class="s3">'score'</span><span class="s1">]</span><span class="s2">,</span>
                                                                                               <span class="s1">ascending=</span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">features = rdm_forest_feature_importance[</span><span class="s3">'feature'</span><span class="s1">][:</span><span class="s4">20</span><span class="s1">]</span>
<span class="s1">scores = rdm_forest_feature_importance[</span><span class="s3">'score'</span><span class="s1">][:</span><span class="s4">20</span><span class="s1">]</span>
<span class="s1">y_pos = np.arange(len(features))</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
<span class="s1">plt.bar(y_pos</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">, </span><span class="s1">align=</span><span class="s3">'edge'</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
<span class="s1">plt.xticks(y_pos</span><span class="s2">, </span><span class="s1">features</span><span class="s2">, </span><span class="s1">rotation=</span><span class="s3">'vertical'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Score'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Features'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Feature importances - Random Forest'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">6.   **Results and Discussion** 
 
 
As a result of the findings obtained and the graphic above, the factors affecting the price the most were determined. Random Forest Algorithm was used and achieved a success rate of 0.964. Among the features that affect the price the most are factors such as the month, the number of toilets, the number of beds, and the number of people. The number of amenities information added with the feature engineering method is effective on the correlation. 
</span><span class="s0">#%% md 
</span><span class="s1">7.   **Conclusion and Future Works** 
 
The purpose of this project is to estimate the preliminary features of the house and the rental price of the person who will create a new AirBnB listing in Seattle, USA. This application can improve by collecting different data such as cleaning fee and using location variables such as latitude and longitude. After the process, it is planned to re-establish this project with the addition of a recomendation bot under the idea that whatever the person who already has an AirBnB listing does can increase the rental price. At the same time, the development of the mobile or web application of the existing project continues. 
 
</span><span class="s0">#%% md 
</span><span class="s1">8.   References 
 
[1]H. Yu and J. Wu, “Real estate price prediction with regression and classiﬁcation,” CS229(Machine Learning) Final Project Reports, 2016. 
 
[2]Y. Ma, Z. Zhang, A. Ihler, and B. Pan, “Estimating warehouse rental price using machinelearning techniques.,” International Journal of Computers, Communications &amp; Control, vol. 13,no. 2, 2018. 
 
[3] Luo, Y. et al. “Predicting Airbnb Listing Price Across Different Cities.” (2019) 
 
[4] Nguyen, QuangTrung. “QuangTrungNguyen/Airbnb-Pricing-Prediction.” GitHub, 
github.com/QuangTrungNguyen/Airbnb-pricing-prediction. 
 
[5] http://insideairbnb.com/</span></pre>
</body>
</html>